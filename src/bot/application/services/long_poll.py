import asyncio
from datetime import datetime
from hashlib import sha256
import json

import aiohttp
from redis.exceptions import ConnectionError as RedisConnectionError
from bot.common.logs import logger
from bot.domain.entities.notification import NotificationTask
from bot.domain.services.long_poll import LongPollServiceInterface
from bot.common.utils.path_parser import (
    parse_datetime,
    build_public_file_url,
    extract_subject_from_path,
    extract_topic_from_path,
    extract_group_from_path,
    extract_group_raw_from_path,
    extract_teacher_from_filename,
    extract_date_from_filename,
    extract_date_from_path,
)


class YandexDiskPollingService(LongPollServiceInterface):
    async def start(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ü–∏–∫–ª –æ–ø—Ä–æ—Å–∞ –ø—É–±–ª–∏—á–Ω–æ–π –ø–∞–ø–∫–∏ –Ø.–î–∏—Å–∫–∞."""
        if self._running:
            return
        self._running = True
        self._task = asyncio.create_task(self._poll_loop(), name="yadisk_poll")
        logger.info("‚úÖ –û–ø—Ä–æ—Å –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–∞ –∑–∞–ø—É—â–µ–Ω")

    async def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ü–∏–∫–ª –æ–ø—Ä–æ—Å–∞ –∏ –¥–æ–∂–¥–∞—Ç—å—Å—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏."""
        self._running = False
        if hasattr(self, "_task") and self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        logger.info("üõë –û–ø—Ä–æ—Å –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

    async def _poll_loop(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–ø—Ä–æ—Å–∞: —Å–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–¥–∞–Ω–∏—è –≤ –æ—á–µ—Ä–µ–¥—å."""
        while self._running:
            try:
                new_files = await self._check_for_new_files()
                if new_files:
                    logger.info(f"üì® –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤: {new_files}")
            except Exception as e:
                logger.exception(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–æ—Å–∞: {e}")

            await asyncio.sleep(self.poll_interval)

    async def _safe_redis_get(self, key: str) -> str | bytes | None:
        try:
            return await self.redis.get(key)
        except RedisConnectionError as e:
            logger.error(f"Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –ø—Ä–∏ GET {key}: {e}")
            return None

    async def _safe_redis_set(self, key: str, value: str, *, ex: int | None = None) -> None:
        try:
            await self.redis.set(key, value, ex=ex)
        except RedisConnectionError as e:
            logger.error(f"Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –ø—Ä–∏ SET {key}: {e}")

    async def _check_for_new_files(self) -> int:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–∏—Å–∫ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã –≤ –æ—á–µ—Ä–µ–¥—å. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á."""
        # –ß–∏—Ç–∞–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç - –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        checkpoint_key = self._get_checkpoint_key()
        last_check = await self._safe_redis_get(checkpoint_key)
        last_check_dt = parse_datetime(last_check)

        # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ —Ç–µ–∫—É—â–µ–≥–æ –æ–±—Ö–æ–¥–∞ (–±–µ–∑ timezone)
        current_check_dt = datetime.now()

        if last_check_dt:
            logger.info(f"üïí –ü–æ—Å–ª–µ–¥–Ω—è—è –ø—Ä–æ–≤–µ—Ä–∫–∞: {last_check_dt.isoformat()}")
        else:
            logger.info("üÜï –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫, —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –Ω–µ—Ç")

        # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≥—Ä—É–ø–ø–∞–º
        new_tasks = []
        group_counts: dict[str, int] = {}
        common_count = 0

        async for file_dict in self._fetch_all_files():
            file_modified = parse_datetime(file_dict.get("modified"))

            # –ü–æ–¥—Å—á—ë—Ç –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            path = file_dict.get("path", "")
            g = extract_group_from_path(path)
            if g:
                group_name: str = str(g)  # StrEnum -> str
                group_counts[group_name] = group_counts.get(group_name, 0) + 1
            else:
                common_count += 1

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã (–º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏)
            if last_check_dt and file_modified and file_modified <= last_check_dt:
                continue

            # –°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á—É –Ω–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
            task = self._create_notification_task(file_dict)
            new_tasks.append(task)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–¥–∞—á–∏ –≤ –æ—á–µ—Ä–µ–¥—å –∏ –æ–±–Ω–æ–≤–ª—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç
        if new_tasks:
            try:
                await self.notification_service.enqueue_many(new_tasks)
            except Exception as e:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–¥–∞—á–∏ –≤ –æ—á–µ—Ä–µ–¥—å: {e}")
            await self._safe_redis_set(checkpoint_key, current_check_dt.isoformat())
            logger.info(f"‚úÖ –ß–µ–∫–ø–æ–∏–Ω—Ç –æ–±–Ω–æ–≤–ª–µ–Ω: {current_check_dt.isoformat()}")
        elif last_check_dt:
            # –î–∞–∂–µ –µ—Å–ª–∏ —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç, –æ–±–Ω–æ–≤–ª—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç
            await self._safe_redis_set(checkpoint_key, current_check_dt.isoformat())
            logger.debug("‚è≠Ô∏è –ù–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç, —á–µ–∫–ø–æ–∏–Ω—Ç –æ–±–Ω–æ–≤–ª–µ–Ω")

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –≥—Ä—É–ø–ø–∞–º (5 –º–∏–Ω—É—Ç)
        try:
            await self._save_group_counts_cache(group_counts, common_count, ttl=300)
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å –∫—ç—à —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≥—Ä—É–ø–ø: {e}")

        return len(new_tasks)

    async def _fetch_all_files(self):
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã —Å –¥–∏—Å–∫–∞ (–æ–±—Ö–æ–¥–∏—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ)."""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–µ–∫ –¥–ª—è –æ–±—Ö–æ–¥–∞ –≤ —à–∏—Ä–∏–Ω—É
        dirs_to_scan = [""]  # –ù–∞—á–∏–Ω–∞–µ–º —Å –∫–æ—Ä–Ω—è

        while dirs_to_scan:
            current_path = dirs_to_scan.pop(0)

            # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            items = await self._fetch_directory(current_path)

            for item in items:
                if item.get("type") == "file":
                    yield item
                elif item.get("type") == "dir":
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ –æ–±—Ö–æ–¥
                    dirs_to_scan.append(item.get("path"))

    async def _fetch_directory(self, path: str) -> list[dict]:
        """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (—Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π)."""
        all_items: list[dict] = []
        offset = 0
        limit = 200

        while True:
            params = {
                "public_key": self.public_root_url,
                "limit": limit,
                "offset": offset,
            }
            if path:
                params["path"] = path

            try:
                timeout = aiohttp.ClientTimeout(total=self.http_timeout)
                url = "https://cloud-api.yandex.net/v1/disk/public/resources"

                async with self.http.get(url, params=params, timeout=timeout) as resp:
                    resp.raise_for_status()
                    data = await resp.json()
                    items = data.get("_embedded", {}).get("items", [])

                    if not items:
                        break

                    # –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                    all_items.extend(items)

                    # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –º–µ–Ω—å—à–µ —á–µ–º limit, –∑–Ω–∞—á–∏—Ç —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                    if len(items) < limit:
                        break

                    offset += limit
                    await asyncio.sleep(0.1)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫—É (path={path}): {e}")
                break

        return all_items

    def _create_notification_task(self, file_dict: dict) -> NotificationTask:
        """–°–æ–∑–¥–∞—ë—Ç –∑–∞–¥–∞—á—É –Ω–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∏–∑ –¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞."""
        path = file_dict.get("path", "")
        file_name = file_dict.get("name", "")
        subject_code = extract_subject_from_path(path)
        study_group = extract_group_from_path(path)
        group_raw = extract_group_raw_from_path(path)
        topic = extract_topic_from_path(path)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä —Ñ–∞–π–ª–∞ –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–µ
        public_url = build_public_file_url(path, self.public_root_url)

        # –ü–∞—Ä—Å–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞/–ø—É—Ç–∏
        teacher = extract_teacher_from_filename(file_name)

        # –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞—Ç—ã –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É: –∏–º—è —Ñ–∞–π–ª–∞ -> –ø—É—Ç—å -> created -> modified
        lesson_date_source = None
        lesson_date = extract_date_from_filename(file_name)
        if lesson_date:
            lesson_date_source = "filename"
        else:
            lesson_date = extract_date_from_path(path)
            if lesson_date:
                lesson_date_source = "path"
            else:
                lesson_date = parse_datetime(file_dict.get("created"))
                if lesson_date:
                    lesson_date_source = "created"
                else:
                    lesson_date = parse_datetime(file_dict.get("modified"))
                    if lesson_date:
                        lesson_date_source = "modified"

        if lesson_date_source:
            logger.debug(
                "üìÖ lesson_date=%s (source=%s) file_name='%s' path='%s'",
                lesson_date.isoformat() if lesson_date else None,
                lesson_date_source,
                file_name,
                path,
            )
        else:
            logger.debug("üìÖ lesson_date not found in filename/path/created/modified for '%s'", file_name)

        return NotificationTask(
            subject_code=subject_code,
            subject_title=None,
            topic=topic,
            study_group=study_group,
            group_raw=group_raw,
            teacher=teacher,
            lesson_date=lesson_date,
            file_name=file_name,
            file_path=path,
            public_url=public_url,
            download_url=file_dict.get("file"),  # –í—Ä–µ–º–µ–Ω–Ω–∞—è —Å—Å—ã–ª–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
            md5=file_dict.get("md5"),
            resource_id=file_dict.get("resource_id"),
            modified_iso=file_dict.get("modified"),
        )

    def _get_checkpoint_key(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á Redis –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–∞."""
        url_hash = sha256(self.public_root_url.encode()).hexdigest()[:12]
        base = f"checkpoint:{url_hash}"
        return f"{self.key_prefix}:{base}" if getattr(self, 'key_prefix', None) else base

    def _group_counts_cache_key(self) -> str:
        url_hash = sha256(self.public_root_url.encode()).hexdigest()[:12]
        base = f"stats:group_counts:{url_hash}"
        return f"{self.key_prefix}:{base}" if getattr(self, 'key_prefix', None) else base

    async def _save_group_counts_cache(self, groups: dict[str, int], common: int, ttl: int = 300) -> None:
        payload = {
            "groups": groups,
            "common": common,
            "computed_at": datetime.now().isoformat(),
        }
        await self._safe_redis_set(self._group_counts_cache_key(), json.dumps(payload), ex=ttl)
