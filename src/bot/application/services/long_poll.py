import asyncio
from datetime import datetime
from hashlib import sha256
from typing import Any, Optional
import json

import aiohttp
from bot.common.logs import logger
from bot.domain.entities.notification import NotificationTask
from bot.domain.services.long_poll import LongPollServiceInterface
from bot.domain.entities.mappings import StudyGroups, TOPICS


class YandexDiskPollingService(LongPollServiceInterface):
    async def start(self):
        if self._running:
            return
        self._running = True
        self._task = asyncio.create_task(self._poll_loop(), name="yadisk_poll")
        logger.info("‚úÖ –û–ø—Ä–æ—Å –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–∞ –∑–∞–ø—É—â–µ–Ω")

    async def stop(self):
        self._running = False
        if hasattr(self, "_task"):
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        logger.info("üõë –û–ø—Ä–æ—Å –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

    async def _poll_loop(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–ø—Ä–æ—Å–∞."""
        while self._running:
            try:
                new_files = await self._check_for_new_files()
                if new_files:
                    logger.info(f"üì® –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤: {new_files}")
            except Exception as e:
                logger.exception(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–æ—Å–∞: {e}")

            await asyncio.sleep(self.poll_interval)

    async def _check_for_new_files(self) -> int:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–∏—Å–∫ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã –≤ –æ—á–µ—Ä–µ–¥—å."""
        # –ß–∏—Ç–∞–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç - –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        checkpoint_key = self._get_checkpoint_key()
        last_check = await self.redis.get(checkpoint_key)
        last_check_dt = self._parse_datetime(last_check)

        # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ —Ç–µ–∫—É—â–µ–≥–æ –æ–±—Ö–æ–¥–∞ (–±–µ–∑ timezone)
        current_check_dt = datetime.now()

        if last_check_dt:
            logger.info(f"üïí –ü–æ—Å–ª–µ–¥–Ω—è—è –ø—Ä–æ–≤–µ—Ä–∫–∞: {last_check_dt.isoformat()}")
        else:
            logger.info(f"üÜï –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫, —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –Ω–µ—Ç")

        # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≥—Ä—É–ø–ø–∞–º
        new_tasks = []
        group_counts: dict[str, int] = {}
        common_count = 0

        async for file_dict in self._fetch_all_files():
            file_modified = self._parse_datetime(file_dict.get("modified"))

            # –ü–æ–¥—Å—á—ë—Ç –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            path = file_dict.get("path", "")
            g = self._extract_group_from_path(path)
            if g:
                name = g.value
                group_counts[name] = group_counts.get(name, 0) + 1
            else:
                common_count += 1

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã (–º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏)
            if last_check_dt and file_modified and file_modified <= last_check_dt:
                continue

            # –°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á—É –Ω–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
            task = self._create_notification_task(file_dict)
            new_tasks.append(task)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–¥–∞—á–∏ –≤ –æ—á–µ—Ä–µ–¥—å –∏ –æ–±–Ω–æ–≤–ª—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç
        if new_tasks:
            await self.notification_service.enqueue_many(new_tasks)
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –æ–±—Ö–æ–¥–∞ –∫–∞–∫ –Ω–æ–≤—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç
            await self.redis.set(checkpoint_key, current_check_dt.isoformat())
            logger.info(f"‚úÖ –ß–µ–∫–ø–æ–∏–Ω—Ç –æ–±–Ω–æ–≤–ª–µ–Ω: {current_check_dt.isoformat()}")
        elif last_check_dt:
            # –î–∞–∂–µ –µ—Å–ª–∏ —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç, –æ–±–Ω–æ–≤–ª—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç
            await self.redis.set(checkpoint_key, current_check_dt.isoformat())
            logger.debug(f"‚è≠Ô∏è –ù–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç, —á–µ–∫–ø–æ–∏–Ω—Ç –æ–±–Ω–æ–≤–ª–µ–Ω")

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –≥—Ä—É–ø–ø–∞–º (5 –º–∏–Ω—É—Ç)
        try:
            await self._save_group_counts_cache(group_counts, common_count, ttl=300)
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å –∫—ç—à —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≥—Ä—É–ø–ø: {e}")

        return len(new_tasks)

    async def _fetch_all_files(self):
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã —Å –¥–∏—Å–∫–∞ (–æ–±—Ö–æ–¥–∏—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ)."""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–µ–∫ –¥–ª—è –æ–±—Ö–æ–¥–∞ –≤ —à–∏—Ä–∏–Ω—É
        dirs_to_scan = [""]  # –ù–∞—á–∏–Ω–∞–µ–º —Å –∫–æ—Ä–Ω—è

        while dirs_to_scan:
            current_path = dirs_to_scan.pop(0)

            # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            items = await self._fetch_directory(current_path)

            for item in items:
                if item.get("type") == "file":
                    yield item
                elif item.get("type") == "dir":
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ –æ–±—Ö–æ–¥
                    dirs_to_scan.append(item.get("path"))

    async def _fetch_directory(self, path: str) -> list[dict]:
        """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (—Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π)."""
        all_items = []
        offset = 0
        limit = 200

        while True:
            params = {
                "public_key": self.public_root_url,
                "limit": limit,
                "offset": offset,
            }
            if path:
                params["path"] = path

            try:
                timeout = aiohttp.ClientTimeout(total=self.http_timeout)
                url = "https://cloud-api.yandex.net/v1/disk/public/resources"

                async with self.http.get(url, params=params, timeout=timeout) as resp:
                    resp.raise_for_status()
                    data = await resp.json()
                    items = data.get("_embedded", {}).get("items", [])

                    if not items:
                        break

                    all_items.extend(items)

                    # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –º–µ–Ω—å—à–µ —á–µ–º limit, –∑–Ω–∞—á–∏—Ç —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                    if len(items) < limit:
                        break

                    offset += limit
                    await asyncio.sleep(0.1)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫—É (path={path}): {e}")
                break

        return all_items

    def _create_notification_task(self, file_dict: dict) -> NotificationTask:
        """–°–æ–∑–¥–∞—ë—Ç –∑–∞–¥–∞—á—É –Ω–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∏–∑ –¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞."""
        path = file_dict.get("path", "")
        file_name = file_dict.get("name", "")
        subject_code = self._extract_subject_from_path(path)
        study_group = self._extract_group_from_path(path)
        group_raw = self._extract_group_raw_from_path(path)
        topic = self._extract_topic_from_path(path)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä —Ñ–∞–π–ª–∞ –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–µ
        public_url = self._build_public_file_url(path)

        # –ü–∞—Ä—Å–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
        teacher = self._extract_teacher_from_filename(file_name)
        lesson_date = self._extract_date_from_filename(file_name) or self._parse_datetime(file_dict.get("modified"))

        return NotificationTask(
            subject_code=subject_code,
            subject_title=None,
            topic=topic,
            study_group=study_group,
            group_raw=group_raw,
            teacher=teacher,
            lesson_date=lesson_date,
            file_name=file_name,
            file_path=path,
            public_url=public_url,
            download_url=file_dict.get("file"),  # –í—Ä–µ–º–µ–Ω–Ω–∞—è —Å—Å—ã–ª–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
            md5=file_dict.get("md5"),
            resource_id=file_dict.get("resource_id"),
            modified_iso=file_dict.get("modified"),
        )

    def _build_public_file_url(self, file_path: str) -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ —Ñ–∞–π–ª –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–µ"""
        import urllib.parse

        # –£–±–∏—Ä–∞–µ–º –≤–µ–¥—É—â–∏–π —Å–ª—ç—à
        clean_path = file_path.lstrip("/")

        # –†–∞–∑–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤—É—é —Å—Å—ã–ª–∫—É –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        base_url = self.public_root_url.split('?')[0].rstrip('/')

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ path
        encoded_path = urllib.parse.quote(clean_path, safe="/")

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Å—ã–ª–∫—É —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º path
        return f"{base_url}/{encoded_path}"

    def _extract_subject_from_path(self, path: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–¥ –ø—Ä–µ–¥–º–µ—Ç–∞ –∏–∑ –ø—É—Ç–∏ (–∏—â–µ—Ç –≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ö –ø—É—Ç–∏)."""
        try:
            from bot.domain.entities.mappings import SUBJECTS

            # –†–∞–∑–±–∏–≤–∞–µ–º –ø—É—Ç—å –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã –∏ –∏—â–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–µ–¥–º–µ—Ç
            segments = [s for s in path.replace("\\", "/").split("/") if s]
            for segment in reversed(segments):
                if segment in SUBJECTS:
                    return segment
        except Exception:
            pass

        return None

    def _extract_topic_from_path(self, path: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–º—É –∑–∞–Ω—è—Ç–∏—è (–õ–µ–∫—Ü–∏—è/–°–µ–º–∏–Ω–∞—Ä) –∏–∑ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –ø—É—Ç–∏."""
        try:
            segments = [s.strip() for s in path.replace("\\", "/").split("/") if s]
            for segment in segments:
                if segment in TOPICS:
                    return segment
        except Exception:
            pass
        return None

    def _extract_group_from_path(self, path: str) -> Optional[StudyGroups]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–¥ —É—á–µ–±–Ω–æ–π –≥—Ä—É–ø–ø—ã –∏–∑ –ø—É—Ç–∏, –µ—Å–ª–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –ø–∞–ø–∫–∞.
        –ù–∞–ø—Ä–∏–º–µ—Ä: '/1 –∫—É—Ä—Å/–ú–ê/–ë–ö–ù–ê–î252/...' -> StudyGroups.BKNAD252
        –õ–µ–∫—Ü–∏–∏ –æ–±—â–∏–µ –¥–ª—è –∫—É—Ä—Å–∞ –æ–±—ã—á–Ω–æ –±–µ–∑ –ø–∞–ø–∫–∏ –≥—Ä—É–ø–ø—ã: '/1 –∫—É—Ä—Å/–õ–ê/–õ–µ–∫—Ü–∏—è/...'
        """
        try:
            segments = [s for s in path.replace("\\", "/").split("/") if s]
            values = set(g.value for g in StudyGroups)
            for segment in segments:
                if segment in values:
                    # –í–µ—Ä–Ω—ë–º enum –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é
                    return StudyGroups(segment)
        except Exception:
            pass
        return None

    def _extract_group_raw_from_path(self, path: str) -> Optional[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –≤ –ø—É—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç, –ø–æ—Ö–æ–∂–∏–π –Ω–∞ –∫–æ–¥ –≥—Ä—É–ø–ø—ã (–¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω enum).
        –ü–∞—Ç—Ç–µ—Ä–Ω: ^–ë–ö–ù–ê–î\d{3}$
        """
        import re

        segments = [s for s in path.replace("\\", "/").split("/") if s]
        pattern = re.compile(r"^–ë–ö–ù–ê–î\d{3}$", re.IGNORECASE)
        for segment in segments:
            if pattern.match(segment):
                return segment
        return None

    def _extract_teacher_from_filename(self, filename: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º—è –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞.
        –§–æ—Ä–º–∞—Ç: '–§–∞–º–∏–ª–∏—è –ò.–û. 2025-10-15T08-08-19Z.mp4'
        """
        import re

        # –ü–∞—Ç—Ç–µ—Ä–Ω: –§–∞–º–∏–ª–∏—è –ò.–û. (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞ + —Ç–æ—á–∫–∏)
        # –ü—Ä–∏–º–µ—Ä—ã: "–õ–æ–±–æ–¥–∞ –ê.–ê.", "–ú–µ–¥–≤–µ–¥—å –ù.–Æ.", "–û–≤—á–∏–Ω–Ω–∏–∫–æ–≤ –°.–ê."
        pattern = r'^([–ê-–Ø–Å–∞-—è—ë]+\s+[–ê-–Ø–Å]\.[–ê-–Ø–Å]\.)'
        match = re.match(pattern, filename)

        if match:
            return match.group(1).strip()

        return None

    def _extract_date_from_filename(self, filename: str) -> Optional[datetime]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –∑–∞–Ω—è—Ç–∏—è –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞.
        –§–æ—Ä–º–∞—Ç: '2025-10-15T08-08-19Z' –∏–ª–∏ '2025-10-15'
        """
        import re

        # –ü–∞—Ç—Ç–µ—Ä–Ω: ISO-–ø–æ–¥–æ–±–Ω–∞—è –¥–∞—Ç–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
        # –§–æ—Ä–º–∞—Ç: YYYY-MM-DDTHH-MM-SSZ –∏–ª–∏ YYYY-MM-DD
        pattern = r'(\d{4})-(\d{2})-(\d{2})(?:T(\d{2})-(\d{2})-(\d{2})Z?)?'
        match = re.search(pattern, filename)

        if match:
            year, month, day = int(match.group(1)), int(match.group(2)), int(match.group(3))
            hour = int(match.group(4)) if match.group(4) else 0
            minute = int(match.group(5)) if match.group(5) else 0
            second = int(match.group(6)) if match.group(6) else 0

            try:
                return datetime(year, month, day, hour, minute, second)
            except ValueError:
                pass

        return None

    def _parse_datetime(self, value: Any) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏—Ç datetime –∏–∑ —Å—Ç—Ä–æ–∫–∏ –∏–ª–∏ bytes (–±–µ–∑ timezone)."""
        if not value:
            return None

        if isinstance(value, (bytes, bytearray)):
            value = value.decode()

        try:
            # –£–±–∏—Ä–∞–µ–º timezone –∏–∑ ISO —Ñ–æ—Ä–º–∞—Ç–∞
            dt = datetime.fromisoformat(value.replace("Z", "+00:00"))
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º naive datetime (–±–µ–∑ timezone)
            return dt.replace(tzinfo=None)
        except Exception:
            return None

    def _get_checkpoint_key(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á Redis –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–∞."""
        url_hash = sha256(self.public_root_url.encode()).hexdigest()[:12]
        base = f"checkpoint:{url_hash}"
        return f"{self.key_prefix}:{base}" if getattr(self, 'key_prefix', None) else base

    def _group_counts_cache_key(self) -> str:
        url_hash = sha256(self.public_root_url.encode()).hexdigest()[:12]
        base = f"stats:group_counts:{url_hash}"
        return f"{self.key_prefix}:{base}" if getattr(self, 'key_prefix', None) else base

    async def _save_group_counts_cache(self, groups: dict[str, int], common: int, ttl: int = 300) -> None:
        payload = {
            "groups": groups,
            "common": common,
            "computed_at": datetime.now().isoformat(),
        }
        await self.redis.set(self._group_counts_cache_key(), json.dumps(payload), ex=ttl)
